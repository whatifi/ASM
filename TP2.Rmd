---
title: "TP2 Cancer de Prostate"
authors: "Amine Zarbi / Siham Tassouli / Wafae Hatifi"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# I) Analyse préliminaire des données de prostate
#### Question 1.a:


```{r echo=FALSE, warning=FALSE}
dataCancer=read.table("prostate.data",header=TRUE)
data = dataCancer[-ncol(dataCancer)] 
pro = as.data.frame(cbind(  data[,1],scale(data[,2:4]),data[,5],scale(data[,6]),data[,7],scale(data[,8:9] ) ))
names(pro) = names(data)
pairs(pro, labels =colnames(dataCancer), pch = 21,bg =c("turquoise"))
```

Si on visualise le graphe des correlations par pairs entre lcavol et  les autres predicteurs, On peut remarquer que le nuage de points entre (\(\textit{lcavol}\), \(\textit{lpsa}\))  et (\(\textit{lcavol}\), \(\textit{lcp}\)) se regroupe autour d'une ligne, en soulignant  un fort alignement  pour le nuage de points (\(\textit{lcavol}\), \(\textit{lpsa}\)). Pour la visualisation des autres nuages de points  entre lcavol et les autres prédicteurs quantitatifs et qualitatifs, on peut pas déduire une information sur le degré de corrélation.

#### Question 1.b:

La visualisation par paires des corrélations permet d'avoir une idée sur la dépedance  entre (\(\textit{lcavol}\), \(\textit{lpsa}\))  et (\(\textit{lcavol}\), \(\textit{lcp}\)), vu que ça suit une relation classique: dans ce cas une fonction affine. Cependant, on peut en déduire un idée sur la dépendance entre \(\textit{lcavol}\) et les autres prédicteurs;  C'est ce qui nous pousse à  utiliser la commande R <cor> qui  determine le coefficient de correlation entre deux grandeurs on obtient le tableau ci-dessous:

```{r figurename, echo=FALSE, fig.cap=" ", out.width = '90%'}
knitr::include_graphics("Correla9predicteurs.png")
```
On peut remarquer que certains prédicteurs sont corrélés entre eux:

\item 
1)svi est corrélé à \(\textit{lcavol}\) ; lcp est corrélé à \(\textit{lcavol}\) ; \(\textit{lcavol}\) est coréllé à \(\textit{svi}\).

Ainsi \(\textit{lcp}\) a un impact sur la dépendance entre \(\textit{svi }\) et \(\textit{lcavol}\) et aussi  sur la dépendance entre \(\textit{gleason }\) et \(\textit{lcavol}\).

2)Dépendence très faible entre \(\textit{lcavol}\) et \(\textit{lweight}\) et \(\textit{age}\). Ainsi qu'entre  \(\textit{lbph}\) et tous les autres préditeurs.

# II) Régression linéaire
#### a) Equation mathématique de la régression linéaire et explications du modèle:
Les deux variables \(\textit{svi}\)  et \(\textit{gleason}\)  sont considérées ci-dessous comme étant des variables qualitatifs.
\(\textit{svi}\) posséde deux niveaux de modalité et \(\textit{gleason}\) posséde 4 niveaux de modalités.
Le modéle initiale peut se mettre de cette manière:


$Y_i =  \alpha_0 + \beta_1lweight_i + \beta_2age_i  + \beta_3lbph_i + \beta_{40}\mathbb 1_{svi = 0} + \beta_{41}\mathbb 1_{svi = 1} + \beta_5lcp_i + \beta_{6}\mathbb 1_{gleason = 6} + \beta_{7}\mathbb 1_{gleason = 7}+ \beta_{8}\mathbb 1_{gleason = 8} + \beta_{9}\mathbb 1_{gleason = 9} + \beta_{10}pgg45_i + \varepsilon_i$



En faisant deux  reparamétrisations  faisant apparaître des effets différentiels par rapport à deux  niveaux de référence successifs  qu'est \(<\textit{gleason} = 6>\) et \(<\textit{svi} = 0>\) dans ce cas on obtient:

 \(Y_i =  \beta_0 + \beta_1lweight_i + \beta_2age_i  + \beta_3lbph_i + \beta_4\mathbb 1_{svi = 1} + \beta_5lcp_i +  \beta_{7}\mathbb 1_{gleason = 7}+ \beta_{8}\mathbb 1_{gleason = 8} + \beta_{9}\mathbb 1_{gleason = 9} + \beta_{10}pgg45_i + \varepsilon_i\)

Ainsi que les valeurs de \(\textit{gleason7}\) et \(\textit{gleason8}\) et \(\textit{gleason9}\)  sont relatifs au niveau de référence \(<\textit{gleason} = 6>\) .


#### b) Summary:
```{r  echo=FALSE, warning=FALSE }
###########Regression de lcavol sur tous les autres predicteurs##############
proC = pro
n = dim(proC)[1]
proC$svi = factor(proC$svi)
proC$gleason = factor(proC$gleason)
lcavol = proC$lcavol
lmLcavol= lm(lcavol ~ . ,data=proC[,-1])
s = summary(lmLcavol)
s
```


#### c) Elimination de lcp et lpsa du modèle de regression:

En eliminant \(\textit{lcp}\)  et \(\textit{lpsa}\)  de notre modèle de regression on remarque que la quasi-totalité de l'information de \(\textit{lcavol}\)  sera dans \(\textit{svi}\)  et \(\textit{gleason}\) , ce qui raisonable car dans la partie 1) on a souligné la forte corrélation entre \(\textit{lcp}\)  et \(\textit{svi}\)  et \(\textit{svi}\)  et \(\textit{lpsa}\)  d'une part et entre \(\textit{lcp}\)  et \(\textit{gleason}\)  d'autre part. Ce transfert d'information est dû à la forte correlation de \(\textit{lcavol}\)  avec \(\textit{lpsa}\)  et \(\textit{lcp}\) .

Ceci confirmé en simulant ce nouvau modèle: on obtient une \(p-value  = 0.000291\) pour \(\textit{svi}\). 

De plus le coefficient de corrélation de ce nouveau modèle a diminué  de 0.6865 à 0.4327, Le modèle est moins cohérent que le premier, Ce qui est logique car \(\textit{lpsa}\)  et \(\textit{lcp}\) sont les plus corréllées à  \(\textit{lcavol}\).
```{r echo=FALSE, warning=FALSE }
###########On enleve lpsa et lcp de la ref=gression##############
proC = pro
n = dim(proC)[1]
proC$svi = factor(proC$svi)
proC$gleason = factor(proC$gleason)
lcavol = proC$lcavol
lmLcavol= lm(lcavol ~ proC$svi+proC$gleason+lweight+age+lbph+pgg45 ,data=proC[,-1])
s = summary(lmLcavol)
s
```

#### d) Impact de lcp sur le modèle de la question1:
```{r echo=FALSE, warning=FALSE}

stdLCP = 0.118954
betaLcp = 0.563158
x = 86
qtStudent = qt(1- 0.05/2, x )
partiegauche = betaLcp - stdLCP*qtStudent
partieDroite = betaLcp + stdLCP*qtStudent

```
Intervalle de Confiance: #latex

En utilisant R On obtient:  \([0.3266853, 0.7996307]\): On remarque que 0 n'appartient pas à l'intervalle de confiance. et donc On peut rejetter l'hypothèse nulle qu'est \(\beta = 0 \).

Conclusion: On obtient une \(p-value  = 8.58e-06\) pour \(\textit{lcp}\) qui est très faible, ce qui soutenu par l'intervalle de confiance étroit.

#### e) Distribution de la statistique t et cohérence des variables indiquées *:

```{r echo=FALSE, warning=FALSE}
#2e)
t<-qt(1-(0.05)/2, 86)

```
Sous l'hypothèse nulle: La distribution de la probabilité de t est celle de la distribution de la loi de student.

Le paramètre de la loi de student est égale à $n -p - 1 = 97 -10 -1 = 86$.

Avec:  $n$ = nombre d'obsevations, $p$ = nombre de variables.

La valeur de t pour laquel $P = 0.05$ est égale à \({F_{st(86)} }^{-1}(1-0.05/2)\) = 1.987608 ou -1.987608

La variable \(\textit{age}\) est notée $*$, Ce qui est cohérent car sa \(|t-value| = |2.205| = 2.205 > |t| = 1.987608 \).

La variable \(\textit{pgg45}\) est notée $*$, Ce qui est cohérent car sa \(|t-value| = |-2.080| = 2.080 > |t| = 1.987608 \).



#### f) Visualisation des lcavol estimés et résidus:
 
```{r echo=FALSE, warning=FALSE}
ValeursActuelles = data[,1]
ValeursEstimees = fitted(lmLcavol)
plot(ValeursEstimees, ValeursActuelles, col = "brown4" ,main = "Valeurs de lcavol estimées en fonction des Valeurs actuelles.")
y1<-sort.int(fitted(lmLcavol))
y2<-sort.int(data[,1])
observations = 1:97
plot(observations,y1, col = "blue" , main = "Valeurs de lcavol estimées(bleu) et Actuelles(rouge).")
lines(observations, y2, col="red")

```

L'allure de la densité des résidus converge vers une densité d'une gaussienne, voire la figure de l'histogramme des résidus.
```{r echo=FALSE, warning=FALSE }

hist(lmLcavol$residuals, col = "brown4", border = "khaki4", freq=FALSE , main=" Histogramme des résidus")
par(new=TRUE)
plot(density(lmLcavol$residuals), col="brown1", main="", xlab="", ylab="", xaxt="n", yaxt="n")


```

La valeur RSS vaut: 41.81407

```{r echo=FALSE, warning=FALSE}
rss = sum(lmLcavol$residuals^2)
```



# III) Effet des prédicteurs qualitatifs:

On peut affirmer à l'aide de ANOVA que Les deux prédicteurs qualitatifs: \(\textit{svi}\)  et \(\textit{gleason}\) ont un effet important sur l'information de lcavol , vu que leurs p-value sont faibles :  $2.23e-09$ pour \(\textit{svi}\) et $0.00233$ pour \(\textit{gleason}\).

```{r echo=FALSE, warning=FALSE}
anova <- aov(lcavol ~ factor(svi)*factor(gleason), proC)
model.tables(anova,"means")
summary(anova)
```


# IV) Best Subset Selection:

#### a) la non-optimalité de la partie 2:
La partie 2 n'est pas optimale parce qu'elle ne prend pas en considération les différentes combinaisons possibles entre les prédicteurs.

\(\textit{lm}\)(\(\textit{lcavol}\)~1,data=pro) est une regression de taille 0.

\(\textit{lm}\)(\(\textit{lcavol}\)~.,data=pro[,c(1,4,9)]) est une regression linéaire de tous le prédicteurs par rapport aux variables 1,4 et 9.

\(\textit{lm}\)(\(\textit{lcavol}\)~.,data=pro[,c(1,2,9)]) est une regression linéaire de tous le prédicteurs par rapport aux variables 1,2 et 9.

Pour calculer RSS on utilise la formule rss = sum(lmLcavol$residuals^2).

#### Toutes les regressions de taille 2: 

```{r echo=FALSE, warning=FALSE}
#part4
lm1<-lm(lcavol~1,data=proC)
lm2<-lm(lcavol~.,data=proC[,c(1,4,9)])
lm3<-lm(lcavol~.,data=proC[,c(1,2,9)])
Combine <- combn(proC[,2:9],2)
Combine[,1]
```

#### b) Fonction minimsant RSS pour chaque k:



```{r echo=FALSE, warning=FALSE}
RSSmin <- function(k){
  if( k == 0 ){
    lm1 <- lm(lcavol~1,data=proC)
    RSS <- sum(resid(lm1)^2)
    return(list(RSSmin=RSS,prednames=""))
  }
  else{
    RSS = 1000000000000000
    Combinaisons <- combn(seq(2,9),k)
    for(i in 1:ncol(Combinaisons)){
      combinaison <- union(Combinaisons[,i],1)
      lm <- lm(lcavol~., proC[,combinaison] )
      RS <- sum(resid(lm)^2)
      if(RS < RSS){
        RSS = RS
        i1 = i
      }
    }
    return(list(RSSmin = RSS,prednames=(attributes(proC)$names)[Combinaisons[,i1]]))
    
  }
}
resultat1 <- RSSmin(1)
resultat2 <- RSSmin(2)
resultat3 <- RSSmin(3)
resultat4 <- RSSmin(4)
resultat5 <- RSSmin(5)
resultat6 <- RSSmin(6)
resultat7 <- RSSmin(7)
resultat8 <- RSSmin(8)
resultat1$prednames
resultat2$prednames
resultat3$prednames
resultat4$prednames
resultat5$prednames
resultat6$prednames
resultat7$prednames
resultat8$prednames
```



```{r echo=FALSE, warning=FALSE}
#### RSS en fonction de k:
Yplot <- RSSmin(0)$RSSmin
for(i in 1:8){
  Yplot <- cbind(Yplot, RSSmin(i)$RSSmin)
}
plot(seq(0,8,1), Yplot, xlab = "k", ylab = "RSS minimale", type = "l", main = "RSS minimale pour chaque k " )

```



### c) Ce modèle n'est pas optimal
Si cette méthode est efficace pour prédire le modèle optimale alors on choisira toujours le modèle de taille maximale puisque RSS diminue en augmentant le nombre de prédicteurs.


# V) Split-validation:


#### a) Définition de split-validation:

La Split-validation est une approche standard pour estimer l'erreur de prédiction en consistant à diviser le jeu de données en deux parties:Apprentissage avec n1 individus  et Validation avec n2 individus.

-L’ensemble d'Apprentissage est utilisé pour s’adapter aux modèles, c’est-à-dire pour fournir des estimations des coefficients de régression.\((\hat\beta_0, \hat\beta_1, ...,\hat\beta_p )\)

-Le jeu de validation est utilisé pour estimer l'erreur de prédiction.L'erreur de prédiction est donné par la formule:

\(E_{pred} = \frac{ 1 }{ n_2 }\sum\limits_{(x_i,y_i)\in Validation ~ Set}\left(  y_i - \left(\widehat{\beta}_0 + \sum_{j=1}^px_{ ij }\widehat{ \beta}_j\right)\right)^2\)

Nous choisissons généralement le modèle qui génère la plus petite erreur de prédiction.

```{r echo=FALSE, warning=FALSE}
##################### selection des individus d'indice multiple de 3#######################

valid <- seq(1,nrow(proC),3)

```

#### b) L'erreur d'apprentissage:

L'erreur d'apprentissage est donnée par:
\(sum(lmSplit$residuals**2)/32\)

On obtient: 0.9004703
```{r echo=FALSE, warning=FALSE}
lmSplit <- lm(lcavol~.,data=proC[-valid,c(1,6,9)])
summary(lmSplit)
```
 
```{r echo=FALSE, warning=FALSE }
Ertraining <- sum(lmSplit$residuals**2)/32

```
#### c) L'erreur de prédiction:


Pour calculer l'erreur de prédiction on utilise la formule :

\(E_{pred} = \frac{ 1 }{ n_2 }\sum\limits_{(x_i,y_i)\in Validation ~ Set}\left(  y_i - \left(\widehat{\beta}_0 + \sum_{j=1}^px_{ ij }\widehat{ \beta}_j\right)\right)^2\)



```{r echo=FALSE, warning=FALSE}
prediction <- predict(lmSplit, proC[valid,])
```


```{r echo=FALSE, warning=FALSE }
#### L'erreur de prediction: ######
Erprediction <- sum((proC[valid,1]-prediction)**2)/65
```

L'erreur de prediction est égale à 0.2907757 plus petit que l'erreur d'apprentissage $0.9004703$, Ce qui est logique car le nombre d'individus pour l'apprentissage est plus petit. $(n1 = 32, n2 = 65)$.


#### d) Les 9 differents modèles:

```{r echo=FALSE, warning=FALSE}
Split_validation <- function(k){
  proC$gleason = pro$gleason
  proC$svi = pro$svi
  if (k==0){
    LM <- lm(lcavol ~ 1, proC[-valid,])
    validation<-pro[valid,]
  }  
  else{
    LM <- lm(lcavol ~ ., proC[-valid,union("lcavol",RSSmin(k)$prednames)])
    validation<-pro[valid, union("lcavol",RSSmin(k)$prednames) ]
  }
  prediction <- predict(LM, newdata = validation)
  Ertraining <- sum(LM$residuals**2)/32
  Erprediction <- sum((proC[valid,1]-prediction)**2)/65
  return(list(Ertraining = Ertraining, Erprediction =Erprediction))
} 
Split_validation(5)
```


```{r}
Yplot1 <- Split_validation(0)$Ertraining
Yplot2 <- Split_validation(0)$Erprediction
for(i in 1:8){
  Yplot1 <- cbind(Yplot1,  Split_validation(i)$Ertraining)
  Yplot2 <- cbind(Yplot2,  Split_validation(i)$Erprediction)
}
Yplot3 = Yplot1+Yplot2
plot(seq(0,8,1), Yplot1, type = "n", ylim = range(c(Yplot1, Yplot2, Yplot3)), xlab = " k", ylab = " Erreurs Appren et prédictions",main = "Valeurs de ErreurApprentissage(bleu) et ErreurPrédiction(rouge).")
lines(seq(0,8,1), Yplot1, col = "blue")
lines(seq(0,8,1), Yplot3, col = "pink")
lines(seq(0,8,1), Yplot2, col = "red")
```


Pour l'erreur d'apprentissage le modèle optimale est celui de la taille 8, et pour l'erreur de Prédiction est celui de la taille 1.En faisant la somme des erreurs on déduit que le modèle minimisant l'erreur global est celui de la taille 8.


#### Question e):
Le principal problème de la split-Validation est la multitude de choix pour choisir la valeur de $n_1$ et $n_2$ pour en déduire le modèle optimale.

#### Question f):
On peut éventuellement penser à la méthode de cross-validation.

